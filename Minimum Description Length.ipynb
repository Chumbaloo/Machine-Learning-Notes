{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sending models and residuals\n",
      "----------------------------\n",
      "\n",
      "- Sender and recevier both know X\n",
      "- Want to send y using minimum number of bits\n",
      "- We will send y in two parts:\n",
      "    - the code (model)\n",
      "    - residual (training error)\n",
      "    - Decision tree:\n",
      "        - code = the tree\n",
      "        - residual = the misclassifications\n",
      "    - Linear regression\n",
      "        - code = the weights\n",
      "        - residual = prediction errors\n",
      "        \n",
      "The MDL model is of optimal complexity, trades off bias and variance\n",
      "\n",
      "Sending binary list of errors: (0, 1, 0, 1, 0, 0, 0 .. n) where 1 is an error:\n",
      "\n",
      "- $|y - \\hat{y}|_0 \\log_2(n)$\n",
      "\n",
      "Encoding $n$ Bernoulli variables, the minimal amount of information possible is simply the entropy: \n",
      "\n",
      "- Entropy: $-\\sum_i^n p \\log p$\n",
      "- $n$ Bernoulli variables: $-n (p_1 \\log p_1 + p_0 log p_0)$\n",
      "\n",
      "Now we need to code the model:\n",
      "\n",
      "- for example\n",
      "    - y = 3.1$x_1$ + 97.2$x_{321}$ - 17$x_{5402}$\n",
      "- two part code for the model:\n",
      "    - which features are in the model\n",
      "    - the coefficients of those features\n",
      "    - *how do we code which features are in the model?*\n",
      "    - *how do we code the feature values?*\n",
      "- need to code the residual\n",
      "    - $\\sum_i(y_i - y_i^{est})^2$\n",
      "    - we know that $y$ and $w_i$ are both normally distributed\n",
      "        - $y \\sim N(x\\times w, \\sigma^2)$\n",
      "        - $\\hat{w} \\sim N(w, \\frac{\\sigma^2}{n})$\n",
      "    - we need to code real numbers, but real numbers can require infinite bits to be infinitely accurate\n",
      "        - when we code the residual, we code it only to the accuracy of $\\sigma$, i.e. the noise or absolute variance or uncertainty\n",
      "            - whatever the underlying $y$ may to be, we don't get to code it\n",
      "        - when we code the weights, they only need to be accurate to $\\sim \\sqrt{n}$\n",
      "    - we can code a real number by dividing it into regions of equal probability mass (before we do any estimation)\n",
      "        - we use $\\sigma^2$ and $n$ to decide how many bits to code $\\hat{w}$\n",
      "        - before we see anything, we break the world into chunks. then if distribution of $\\hat{w}$ is huge, then it will be on the outer chunks and we can't send very much information. If distribution of $\\hat{w}$ is small, we can send more information\n",
      "        - the bigger $\\sigma$ is, the fewer bits it makes sense to code\n",
      "        - that way we tell you what region it's in, and that's all\n",
      "        - ** the most efficient code is splitting things up into bins of equal probability**\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regression Penalty Methods\n",
      "-------------------\n",
      "Penalty has two components, presence-absence and weight\n",
      "\n",
      "**Minimize:**  \n",
      "Err/$2\\sigma^2 + \\lambda|w|_0$\n",
      "    - L0 penalty on coefficients \n",
      "    - q = number of features actually in your model = $|\\hat{w}|_0$ = $L_0$ norm of $\\hat{w}$\n",
      "    \n",
      "**Method | penalty ($\\lambda$)**  \n",
      "\n",
      "- AIC (known also as Mallows $C_p$) | 1\n",
      "    - code coefficient using 1 bit (that means you think each feature is equally likely (50%))\n",
      "    - leaves no bit for coding the accuracy of the weights\n",
      "    - it _sends_ the weights, but it does not _charge_ for them\n",
      "    - thus, we are charging too little for a model, and thus will send a model that is _too complex_\n",
      "    - will AIC underfit or overfit?\n",
      "        - we send the weights for no charge, so we send a model that is too complicated, so it will **overfit**\n",
      "    - $C_p = Err_q / \\sigma^2 + n + q$\n",
      "        - same as AIC = $-2 \\log{\\textrm{likelihood}} + 2q$\n",
      "- BIC | $1/2 \\log(n)$\n",
      "    - code coefficient using $\\sqrt{n}$ bits\n",
      "        - this is same as $\\log(\\sqrt{n})$\n",
      "    - if $n$ is big, then coding presence/absence is relatively cheap compared to the weights\n",
      "    - also known as MDL (as $n \\rightarrow \\infty$ )\n",
      "- RIC | $\\log(p)$\n",
      "    - code feature presence/absence\n",
      "    - ideal if sparse, i.e. a lot of features and you're sending a very small fraction of them\n",
      "    \n",
      "There's a $\\sigma^2$ in the error, but we don't know it. We can get a good estimate though  \n",
      "\n",
      "If we've done a good job on our $\\hat{y}$, $(y - \\hat{y}) \\sim N(0, \\sigma^2)$ and this is a decent estimate of $\\sigma^2$. If we overfit $\\hat{y}$ then our $\\sigma^2$ will be near 0 and a bad estimate\n",
      "\n",
      "- If we set $\\hat{y} \\equiv 0$, does it _underestimate_ or _overestimate_ $\\sigma^2$?\n",
      "    - If you do the opposite, and make $\\hat{y} \\equiv y$, then your estimate of the variance decreases\n",
      "    - Therefore a $\\hat{y}$ of 0 means you will _overestimate_ your $\\sigma^2$\n",
      "    - with $\\hat{y} \\equiv 0$, then the distribution will really be $\\sim N(wx, \\sigma^2)$, but we will not subtract the bias and therefore overestimate $\\sigma^2$\n",
      "    - the variance of $y$ is two components: $wx$ and $\\sigma^2$\n",
      "        - $Var(y) = Var(x)^2 + \\sigma^2$\n",
      "\n",
      "Which penalty should you use if:\n",
      "\n",
      "- you expect 10 out of 100k features, n = 100\n",
      "    - RIC\n",
      "    - Under an $L_0$ norm, you can try exponentially many features in a finite number of observations\n",
      "- you expect 200 out of 1,000 features, n = 1,000,000\n",
      "    - AIC might make sense because you have roughly half the features being used\n",
      "    - BIC is better because you want to code the weights accurately since you have a large number of observations\n",
      "    \n",
      "- you expect 500 out of 1,000 features, n = 1,000\n",
      "    - AIC: roughly 50/50 prior, not a huge $n$ so you can't estimate the weight accurately\n",
      "- n = 1 million, p = 1 million, q = 10\n",
      "    - RIC = $10 \\times \\log 10^6, q \\log p$\n",
      "    - BIC = $10 \\times \\log 10^3, q \\log \\sqrt{n}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}