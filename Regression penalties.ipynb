{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Starting notes:**\n",
      "\n",
      "- Review session Monday 10/7, 7:00pm (location TBA, keep eye on Piazza)\n",
      "- Midterm will allow one two-sided cheat sheet\n",
      "    - Midterm questions are going to be like the clicker questions\n",
      "    - Review concepts from the homeworks\n",
      "    - KL divergence, hat matrix, etc\n",
      "    - Covers through today\n",
      "        - HW4 involves coding up NB, Logit Regression, relevant to midterm\n",
      "        - no boosting, no neural networks\n",
      "        \n",
      "**Clicker question:**  \n",
      "Ridge regression (\"Tikhonov regularization\") minimizes $Err + \\lambda|w|_2^2$ \n",
      "Is $Err$ here\n",
      "\n",
      "1.  $\\sum_i (y_i - \\hat{y}_i)^2 \\leftarrow$ \n",
      "2.  $(1/n) \\sum_i (y_i - \\hat{y}_i)^2$\n",
      "3.  $\\sqrt( 1/n \\sum_i (y_i - \\hat{y}_i)^2$\n",
      "4.  $\\sqrt(\\sum_i (y_i - \\hat{y}_i)^2)$ \n",
      "\n",
      "$P(w) \\sim N(0, \\gamma^2) \\\\\n",
      "P(Y | w) \\prod p(w_j) \\\\\n",
      "p(w_j) \\sim e^{-w^2/\\gamma^2} \\\\\n",
      "log \\prod e^{-w^2/\\gamma^2} = \\sum w_i^2 \\equiv |w|^2_2, \\textrm{or, the L2 norm of the weights, squared}$\n",
      "\n",
      "- Both the error term and the w term are L2 norms, squared\n",
      "- Gaussian noise leads to closed-form solutions\n",
      "- Each different penalty can be derived from some prior. $|w|^2_2$ is the gaussian prior\n",
      "\n",
      "Elastic net regularization\n",
      "--------------------------\n",
      "\n",
      "- ENR minimizes $Err + \\lambda_1|w|_1 + \\lambda_2|w|_2^2$\n",
      "- Will this sometimes zero out some features? **clicker question**\n",
      "    - L2 says make every features smaller by a constant factor\n",
      "    - L1 says we want to reduce the *absolute value* of the weight\n",
      "    - L1 is a feature selection method, L2 is not\n",
      "        - Correspondingly, L1 sets some weights to zero\n",
      "    - So, in the above, this will be shrunk by some constant factor, then below a certain threshold it will be zero\n",
      "    - **Answer: yes, because of L1**\n",
      "    - Why do we want both? When is L1 a good thing to have?\n",
      "        - L1 is used for optimization (L0 is not convex, you wind up doing search)\n",
      "        - What about when we have a lot of features that are redundant?\n",
      "            - L1 regression penalizes stupid solutions, \"zeros out\" irrelevant features\n",
      "- ENR is convex and can be solved \n",
      "\n",
      "Now we have four methods to choose features:\n",
      "\n",
      "- L1 (+L2)\n",
      "- L0 (AIC, BIC, RIC)\n",
      "- In a world where you think most things are irrelevant, L1 is a bad idea (?)\n",
      "\n",
      "Ridge Regression\n",
      "------------------\n",
      "Ridge regression is **not** scale invariant  \n",
      "Useful link: [Lp space](http://en.wikipedia.org/wiki/Lp_space)\n",
      "RR: scale big weights more than the small weights\n",
      "\n",
      "\n",
      "Regression Penalty methods\n",
      "----------------------\n",
      "L0 zeros out the one with the least predictive value, the _least_ effect on Y (so by saying small values, he means small effect on Y)\n",
      "\n",
      "AIC, BIC, and RIC minimize $Err/2\\sigma^2 + \\lambda|w|_0$\n",
      "\n",
      "Is this error:  \n",
      "\n",
      "1.  $\\sum_i (y_i - \\hat{y}_i)^2 \\leftarrow$ \n",
      "2.  $(1/n) \\sum_i (y_i - \\hat{y}_i)^2$\n",
      "3.  $\\sqrt( 1/n \\sum_i (y_i - \\hat{y}_i)^2$\n",
      "4.  $\\sqrt(\\sum_i (y_i - \\hat{y}_i)^2)$ \n",
      "\n",
      "Why is there a $2\\sigma^2$ in there?\n",
      "\n",
      "If I have really high $\\sigma$, do I shrink more or less?\n",
      "\n",
      "- The noiser the data is, the more you need to shrink the weights\n",
      "- If you have very very small \\sigma, don't shrink, use the MLE\n",
      "- _[don't quite understand this, need to ask at office hours]_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why does MDL work?\n",
      "-------------------\n",
      "\n",
      "1. we want to tune model complexity\n",
      "2. minimize Test error = training error + penalty\n",
      "    -  training error = bias = bits to code the residual\n",
      "        - $\\sum (y_i - \\hat{y}_i)^2 / 2\\sigma^2 = p(y|\\mathbf{X}) \\log p(y|\\mathbf{X})$ _\n",
      "    - Penalty = variance = bits to code the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exam questions:**\n",
      "\n",
      "You think that maybe 10/100,000 features are significant. Use:\n",
      "\n",
      "- $L_0$ with RIC\n",
      "\n",
      "You think that maybe 500/1000 features will be significant. Do NOT use:\n",
      "\n",
      "- $L_0$ with RIC ($L_2$ is also not great)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}