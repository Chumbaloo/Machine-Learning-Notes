{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Review\n",
      "-------------\n",
      "**Q: What is consistency?**  \n",
      "> A: Asymptotically, if you have infinite data, you can obtain the parameter that generated the data\n",
      "\n",
      "**Q: Frequentist vs Bayesian?**  \n",
      ">A: In the Bayesian world, the parameter is considered to be random.  \n",
      ">>Likelihood: $P(D|\\Theta)$  \n",
      ">>Bayesian Prior: $P(\\Theta|D)$  \n",
      "\n",
      "**Q: Assumptions of Linear Regression?**  \n",
      "> A: $Y = W^TX+\\epsilon\\ \\\\ \n",
      "\\hat{w}=(X^TX)^-1X^TY \\\\\n",
      "\\textrm{i.i.d.}\\ \\epsilon \\sim N(0,\\sigma^2) \\\\\n",
      "$  \n",
      "\n",
      "**Q: Do we really want unbiased estimators?**  \n",
      "> A: Not always  \n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loss function\n",
      "----------------\n",
      "\n",
      "- Squared loss: $L(Y, \\hat{Y}) \\rightarrow (Y-\\hat{Y})^2$  \n",
      "  - So what we want to minimize is the expected value of that loss function $L$  \n",
      "  - $MSE = E[L(Y,\\hat{Y})]$  \n",
      "  - Loss: also known as _risk_ or _risk of the estimator_.  \n",
      "  - $\\int (Y-\\hat{Y})^2\\ P(D)\\ d D \\rightarrow\\ 1/n \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2$\n",
      "\n",
      "- The MSE can be decomposed into a combination of the bias and variance.  \n",
      "  - $MSE = \\textrm{bias}^2 + \\textrm{variance} + \\textrm{noise}$\n",
      "  - $Y = W^\\intercal X+\\epsilon$\n",
      "    - $f(x) = W^\\intercal X$\n",
      "  - $\\hat{f}(X) \\rightarrow \\hat{Y}$\n",
      "  - $E[(Y-\\hat{f}(x))^2] = (\\textrm{substitute Y})\\ E[(f(x_0) + \\epsilon - \\hat{f}(x_0))^2]$\n",
      "    - then, expand the squares and simplify\n",
      "    - $\\textrm{var}(\\epsilon) = E[\\epsilon^2] - [E(\\epsilon)]^2 $\n",
      "    - $\\sigma^2 = E[\\epsilon^2] - 0$\n",
      "    - $\\sigma^2 + (E\\hat{f}(x_0)-f(x_0))^2 + E[\\hat{f}(x_0) - E\\hat{f}(x_0)]$\n",
      "    - $\\sigma^2$ + $\\textrm{bias}^2$ + var\n",
      "      - $f(x_0) = Y\\ \\textrm{without the noise}$\n",
      "      - where is the noise? it is $\\sigma^2$\n",
      "      - the bias term is saying, \"having averaged out the noise from the $Y$s, how far am I on average from the noiseless $Y$s?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Overfitting\n",
      "------------\n",
      "Overfitting = $(\\textrm{error}_{train}(\\hat{w}_1) < \\textrm{error}_{train}(\\hat{w}_2)) \\wedge (\\textrm{error}_{test}(\\hat{w}_1) > \\textrm{error}_{test}(\\hat{w}_2))$  \n",
      "\n",
      "Choosing the sweet spot between test and training error is called _model selection_.\n",
      "\n",
      "Model selection says that you penalize $ERM = 1/n \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2$ by adding a term $\\lambda C(W)$, the complexity penalty or _structure risk minimization_, $SRM$.\n",
      "\n",
      "Let's say you are given a training set, validation set, and test set. You train the model with, say, polynomial 1. Then you select the _SRM_ by finding the lowest _SRM_ on the validation set (?) [The lecture is becoming less clear here]\n",
      "\n",
      "Cross-validation, _n_-fold: We divide our training data into _n_ folds. You hold one of these folds back: i.e, train on 1-9, test on 10. Do this for each fold. You compute the error for each fold and then average the error. Do this whole process for each degree of polynomial, and find the degree that has the lowest error.\n",
      "\n",
      "_Bootstrap_: sampling from the training set, but with replacement. The test set are just those observations that were not sampled.\n",
      "\n",
      "_Ridge regression_:  $\\hat{w} = \\textrm{argmax}_w(Y-Y_w)^2 + \\lambda ||w||^2 \\rightarrow (X^\\intercal X + \\lambda I)^{-1}X^\\intercal Y$  \n",
      "\n",
      "- A higher $\\lambda$ gives you a higher bias, but the point of $\\lambda ||w||^2$ is to control the variance.  \n",
      "- $\\lambda ||w||$ is l1, also known as LASSO http://en.wikipedia.org/wiki/Least_squares#Lasso_method  \n",
      "- $\\lambda ||w||^2$ is ... something\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}