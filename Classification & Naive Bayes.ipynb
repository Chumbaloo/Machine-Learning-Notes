{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classification\n",
      "-----------------------\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "========================\n",
      "\n",
      "Naive Bayes for Text Classification\n",
      "-----------------------------------\n",
      "How do you tell if something is spam?  \n",
      "Pick features out of an email: for us, presence or absence of words  \n",
      "\n",
      "$P(C|D) = \\frac{P(D|C)P(C)}{P(D)}$\n",
      "\n",
      "Not very good for estimating probabilities, but very good at giving likelihood\n",
      "\n",
      "- Conditional independence assumption: features are independent of each other given the class:  $P(X_1\\dots X_5 \\mid C) = P(X_1\\mid C)\\ P(X_2\\mid C)\\dots\\ P(X_5\\mid C)$  \n",
      "- This model is appropriate for binary variables\n",
      "- works well as a classifier, but not as much as a model\n",
      "\n",
      "Is naive Bayes more of a generative or discriminitive model?  \n",
      "\n",
      "- Generative: $P(X, Y)$  \n",
      "- Discriminative: $P(X \\mid Y)$\n",
      "\n",
      "MLE works really poorly because the probability of what you haven't seen == 0. How do we fix this? Use MAP instead of MLE: set a _prior_\n",
      "We add in a very small addition to the count so that multiplying by the probability won't equal zero\n",
      "\n",
      "Use small priors: 0.5 or 1.\n",
      "\n",
      "Alternative: use an _empirical_ prior. Base your priors on data, i.e. frequency of words in another distribution\n",
      "\n",
      "**Technical Note**: Multiplying many small probabilities can lead to an overflow error. Instead, take the logs and sum."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}